{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Change - Project\n",
    "\n",
    "## Topic: Gradient Descent\n",
    "\n",
    "> `numerical` `optimisation`\n",
    "\n",
    "Gradient descent is a general method for finding minima of functions taught is widely used in many fields.\n",
    "Write out the equations for a simple gradient descent method and code an implementation in Python.\n",
    "Locate the minima of an example function who's min you can find using the analytically (that is, as we did\n",
    "in lectures and tutorials). Investigate how the convergence is affected by:\n",
    "\n",
    "1. step size or other parameters in the algorith\n",
    "2. the initial starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: \n",
    "\n",
    "**Name:** Jonathon Belotti\n",
    "\n",
    "**Student ID:** 13791607"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report details the mathematics and computer code implementation of the _Gradient Descent_ optimisation method, and investigates the behaviour of the optimisation process when subject to variance in algorithm paramaters ('step size', iteration number) and variance in the initial starting point. \n",
    "\n",
    "### Report Structure\n",
    "\n",
    "1. [**Introduction to the Pure-Python3 Implementation.**](#1.-Introduction-to-the-Pure-Python3-Implementation)\n",
    "2. [**Introduction to the Gradient Descent method.**](#2.-Introduction-to-the-Gradient-Descent-Method)\n",
    "3. [**Convergence.**](#3.-Convergence)\n",
    "5. [**References**](#References)\n",
    "6. [**Appendix**](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to the Pure-Python3 Implementation\n",
    "\n",
    "To assist in the exploration and communication of the Gradient Descent optimisation technique, a 'from scratch' [pure-Python](https://stackoverflow.com/a/52461357/4885590) implementation of the gradient descent algorithm has been written and is used throughout. It is a 'standalone module', and imports no third-party code. The implementation targets usefulness in learning, not performance, but is fast enough for practical example.\n",
    "\n",
    "To reduce implementation complexity and length, not all differentiable functions are supported by the implementation. Supported functions include:\n",
    "\n",
    "- [Polynomial functions](https://en.wikipedia.org/wiki/Polynomial)\n",
    "\n",
    "\n",
    "The implementation is wholly contained in one Python3 module, `gradient_descent`, and it is available at: https://github.com/thundergolfer/modelling_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "\n",
    "import gradient_descent  # Assumes gradient_descent.py is on PYTHONPATH. For module implementation see appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables** become components in expressions and are used in the differentiation functions. They don't do much else.\n",
    "\n",
    "```python\n",
    "x = Variable(\"x\")\n",
    "```\n",
    "\n",
    "**Expressions** can be evaluated at a point in function space, eg $(x=1, y=2, z=4)$ and can be differentiated with respect to a reference variable. \n",
    "\n",
    "`Expression` is a base class for `ConstantExpression`, `PolynomialExpression`, and `Multiply`.\n",
    "\n",
    "```python\n",
    "class Expression:\n",
    "    def diff(self, ref_var: Optional[Variable] = None) -> Optional[\"Expression\"]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, point: Point) -> float:\n",
    "        raise NotImplementedError\n",
    "```\n",
    "\n",
    "**MultiVariableFunctions** are created by specifying their inputs, `variables`, and composing `Expression` objects by \n",
    "addition (subtraction is handled by negative coefficients on expressions).\n",
    "\n",
    "We can get the gradient of a function and evaluated it at a point in function space, just like `Expression` objects.\n",
    "\n",
    "```python\n",
    "class MultiVariableFunction:\n",
    "    \"\"\"\n",
    "    MultiVariableFunction support the composition of expressions by addition into a\n",
    "    function of multiple real-valued variables.\n",
    "\n",
    "    Partial differentiation with respect to a single variable is supported, as is\n",
    "    evaluation at a Point, and gradient finding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, variables: Set[Variable], expressions: List[Expression]): ...\n",
    "\n",
    "    def gradient(self) -> GradientVector: ...\n",
    "\n",
    "    def diff(self, ref_var: Variable) -> \"MultiVariableFunction\": ...\n",
    "\n",
    "    def evaluate(self, point: Point) -> float: ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Introduction to the Gradient Descent Method\n",
    "\n",
    "Gradient Descent is an iterative optimization process that can be used effectively to find local mimina of differentiable functions, particulary when those functions are convex. When the output of a differentiable function under some set of inputs can be framed as a _cost_, the minimization of this _cost function_ becomes an optimization problem to which the Gradient Descent process can be applied.\n",
    "\n",
    "The \"Deep Neural Networks\" revolution that swept through the 2010s has its foundation in the simple single-layer neural networks first published in the 1980s, and those simple networks were optimized through gradient descent. Thus, a first lesson in understanding today's hottest technological field, Deep Neural Networks, involves going right back to the start and understanding the basic Gradient Descent optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 A Function's Gradient\n",
    "\n",
    "In order to minimise a function's value, we need to ascertain which way we should nudge its inputs to decrease the output value, and we have to be sure than a series of decreases will eventually lead to a minimum (local or global). For differentiable functions, the first-derivative of a function can be the way.\n",
    "\n",
    "For a function of a single variable, $f(x)$, the rate of change at some value $x=a$ is given by the first-derivative $f'(x)$. In the case of $f(x) = x^2 + x$, we know that:\n",
    "\n",
    "$$f'(x) = 2x + 1$$\n",
    "\n",
    "and thus at $f'(1) = 2(1) + 1 = 3$ the function is increasing in output value 'to the right' and decreasing 'to the left'. At $f'(-1) = 2(-1) + 1 = -1$ the function is decreasing in output value 'to the right' and increasing 'to the left;. \n",
    "\n",
    "In either case, we know from the first-derivative which direction to nudge $x$, until we reach $f'(1/2) = 2*(1/2) + 1 = 0$ and we've reached the critical point.\n",
    "\n",
    "\n",
    "But for a multi-variable function there are multiple ways in which to influence the output value and thus multiple dimensions along which a we could change inputs. How can we extend our understanding of the direction of function decrease beyond 'left and right' and into 3-dimensions and more? We use partial derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $z = f(x, y)$, then we have a multi-variable function with partial derivatives:\n",
    "    \n",
    "$$f_x(x_0, y_0) = \\lim_{h \\to 0}\\frac{f(x_0+h, y_0) - f(x_0, y_0)}{h}$$\n",
    "\n",
    "$$f_y(x_0, y_0) = \\lim_{h \\to 0}\\frac{f(x_0, y_0+h) - f(x_0, y_0)}{h}$$\n",
    "\n",
    "with each capturing the rate-of-change with respect to a single variable in our multi-variable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the $x,y$ plane, we can imagine being at some point $(x_0,y_0)$ and nudging away from that point in the plane by the vector $\\mathbf{u} = \\langle a, b \\rangle $. \n",
    "\n",
    "Now not restricted to moving 'left and right' in the x-axis or 'up and down' the y-axis, we have a **Directional Derivative** of $f$ at $(x_0,y_0)$.\n",
    "\n",
    "$$D_uf(x_0, y_0) = \\lim_{h \\to 0}\\frac{f(x_0 + ha, y_0+hb) - f(x_0, y_0)}{h}$$\n",
    "\n",
    "More intuitively, we can consider that nudge as being of length $h$ at some angle $\\theta$ (capturing direction). Thus our $a$ and $b$ are $\\cos{\\theta}$ and $\\sin{\\theta}$ respectively, and $\\mathbf{u} = \\langle a, b \\rangle $ is a vector of length 1.\n",
    "\n",
    "In fact, any differentiable function of $x$ and $y$ has a directional derivative in a direction of $\\mathbf{u}$ and this relationship can be expressed as:\n",
    "\n",
    "$$D_uf(x, y) = f_x(x, y)a + f_y(x, y)b$$\n",
    "\n",
    "**To prove this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function $g$ of the single variable $h$ as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(h) & = f(x_0 + ha, y_0+hb) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "By definition of the derivative:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(0) & = \\lim_{h \\to 0}\\frac{g(h) - g(0)}{h} \\\\\n",
    "& = \\lim_{h \\to 0}\\frac{f(x_0+ha, y_0+hb) - f(x_0, y_0)}{h}\\\\\n",
    "& = D_uf(x_0, y_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Writing $x = x_0 + ha$ and $y = y_0 + hb$ we get $g(h) = f(x, y)$ and \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(h) & = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial h} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial h} \\\\\n",
    "& = f_x(x, y)a + f_y(x, y)b \\\\\n",
    "& = D_uf(x_0, y_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Substituting in $h=0$ then $x$ and $y$ become $x = x_0$, $y = y_0$, so:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(0) & = f_x(x, y)a + f_y(x, y)b \\\\\n",
    " & = f_x(x_0, y_0)a + f_y(x_0, y_0)b \\\\\n",
    "& = D_uf(x_0, y_0)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$D_uf(x, y) = f_x(x, y)a + f_y(x, y)b$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relationship between partial derivatives and a directional nudges in each input dimension generalises beyond 2 dimensions, and can be compactly represented by _vectorising_ the combination of the partial derivatives and an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "D_uf(x,y) & = f_x(x, y)a + f_y(x, y)b \\\\\n",
    " & = \\langle f_x(x,y), f_y(x, y) \\rangle \\cdot \\langle a, b \\rangle \\\\\n",
    " & = \\langle f_x(x,y), f_y(x, y) \\rangle \\cdot \\mathbf{u} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the `gradient_descent` library, we can calculate the gradient vector from a `MultiVariableFunction`. For the function:**\n",
    "\n",
    "$$f(x,y) = x^2 + y^2 - 2x - 6y + 14$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gradient_descent.Variable(\"x\")\n",
    "y = gradient_descent.Variable(\"y\")\n",
    "f = gradient_descent.MultiVariableFunction(\n",
    "    variables={x, y},\n",
    "    expressions=[\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=1, exponent=2),\n",
    "        gradient_descent.PolynomialExpression(variable=y, coefficient=1, exponent=2),\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=-2, exponent=1),\n",
    "        gradient_descent.PolynomialExpression(variable=y, coefficient=-6, exponent=1),\n",
    "        gradient_descent.ConstantExpression(real=14.0),\n",
    "    ],\n",
    ")\n",
    "\n",
    "f.gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Steepest Descent\n",
    "\n",
    "Now able to determine the gradient vector of a function, capturing the rate of change along each dimension of a function, the question becomes in which 'direction' to go to 'descend' or decrease the function's value?.\n",
    "\n",
    "$$\\nabla_{u} f(x_0,y_0) = \\mathbf{u} \\cdot f(x_0, y_0)$$ \n",
    "\n",
    "Firstly, if the gradient is zero\n",
    "\n",
    "$$\\nabla_{u} f(x_0,y_0) = 0$$ \n",
    "\n",
    "then the directional gradient is zero in every direction. This would be the end of our descent. But for a non-zero gradient, then the gradient itself is the direction that maximises the dot product.\n",
    "\n",
    "$$\\max \\nabla f(x_0, y_0) \\cdot \\mathbf{u} = \\frac{\\nabla f(x_0, y_0)}{\\lvert \\nabla f(x_0, y_0) \\rvert}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](vector_projection.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually great, because in order to descend fastest from some point $f(x_0, y_0)$ we don't need to calculate which direction is best, the direction of the gradient is the best direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Gradient Descent - Iterating Towards the Bottom\n",
    "\n",
    "Now with a method to calculate the direction of maximum descent from a point $\\mathbf{a}$ in a function's input space, we are very close to creating the _Gradient Descent_ optimisation process.\n",
    "\n",
    "Given a differentiable multi-variable function $f(\\mathbf{x})$, with $\\mathbf{x}$ being a vector of inputs $\\langle x, y, z, ... \\rangle$, then we know:\n",
    "\n",
    "**At some point $\\mathbf{a} \\in \\mathbf{x}$, $f(\\mathbf{x})$ decreases _fastest_ in the direction of the negative gradient:** $-\\nabla \\mathbf{f(a)}$\n",
    "\n",
    "In the Python library `gradient_descent`, we can calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_grad = f.gradient()\n",
    "\n",
    "print(f\"Gradient: {f_grad}\")\n",
    "\n",
    "a: gradient_descent.Point = {\n",
    "    x: -1,\n",
    "    y: 1,\n",
    "}\n",
    "\n",
    "f_grad_a: Mapping[gradient_descent.Variable, float] = {\n",
    "    var: grad_elem.evaluate(a)\n",
    "    for var, grad_elem\n",
    "    in f_grad.items()\n",
    "}\n",
    "    \n",
    "print(\"Gradient of f(x, y) @ point 'a'\")\n",
    "print(f_grad_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4  Analytical vs. Iterative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, understanding the process, we are ready to run Gradient Descent in Python. The optimisation problem we'll solve is minimising:\n",
    "\n",
    "$$cost= f(x,y) = x^2 + y^2 - 2x - 6y + 14$$\n",
    "\n",
    "We can solve this analytically, which will be useful in validating the Python implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "f_x(x, y) & = 2x + 0 - 2 - 0 + 0 \\\\\n",
    "f_x(x, y) & = 2x - 2\\\\\n",
    "\\\\\n",
    "f_y(x, y) & = 0 + 2y - 0 - 6 + 0 \\\\\n",
    "f_y(x, y) & = 2y - 6\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Solving...\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_x(x, y) & = 2x - 2 = 0 \\\\\n",
    "2x - 2 & = 0 \\\\\n",
    "2x & = 2 \\\\\n",
    "x & = 1 \\\\\n",
    "\\\\\n",
    "f_y(x, y) & = 2y - 6 = 0 \\\\\n",
    "2y - 6 & = 0 \\\\\n",
    "2y & = 6 \\\\\n",
    "y & = 3 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So $f(x, y)$ has a critical point at $(1, 3)$ and we can show graphically that this critical point is a minimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Create boundaries of f(x,y)\n",
    "x = np.linspace(-1,15)\n",
    "y = np.linspace(-1,15)\n",
    "\n",
    "# Create 2D domain of f(x,y)\n",
    "xf, yf = np.meshgrid(x,y)\n",
    "\n",
    "# Discrete version of f(x,y) over 2D domain\n",
    "fxy = (xf**2) + (yf**2) - (2*xf) - (6*yf) + 14\n",
    "\n",
    "# Plot our function to be optimised f(x,y)\n",
    "ax.plot_surface(xf, yf, fxy, alpha=0.1)\n",
    "ax.view_init(15, 20)\n",
    "\n",
    "# Plot extrema\n",
    "x_extrema = np.array([1])\n",
    "y_extrema = np.array([3])\n",
    "z_extrema = (x_extrema**2) + (y_extrema**2) - (2*x_extrema) - (6*y_extrema) + 14\n",
    "\n",
    "ax.scatter(x_extrema, y_extrema, z_extrema,  color='r', label='maxima')\n",
    "\n",
    "ax.grid(False)\n",
    "ax.legend()\n",
    "# plt.locator_params(nbins=6) # Amount of numbers per axis\n",
    "# Label our axes\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_zlabel(\"z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's solve the same problem iteratively using the Python implementation of the Gradient Descent algorithm:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core of the iterative gradient descent process is nice and simple:\n",
    "\n",
    "$$ \\mathbf{a}_{n+1} = \\mathbf {a}_{n}-\\gamma \\nabla f(\\mathbf {a} _{n})$$\n",
    "\n",
    "\n",
    "$$f(\\mathbf{a_0}) \\ge f(\\mathbf{a_1}) \\ge f(\\mathbf{a_1}) \\ge f(\\mathbf{a_2}) \\ge f(\\mathbf{a_3}) ...$$\n",
    "\n",
    "This process has 4 clear inputs:\n",
    "\n",
    "1. The function\n",
    "2. The initial starting point, $\\mathbf{a_0}$\n",
    "3. The step size, $\\gamma$ (gamma)\n",
    "4. The number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_val, minimum_point = gradient_descent.gradient_descent(\n",
    "        gamma=0.1,\n",
    "        max_iterations=5000,\n",
    "        f=f,\n",
    "    )\n",
    "print(\"\\nResu\")\n",
    "print(f\"Min Value: {minimum_val}\")\n",
    "print(f\"Min Location: {minimum_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! The answers are not exact because of [floating-point arithmetic error](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html), but $(1, 3)$ is correct.\n",
    "\n",
    "We can re-run the function and no matter which values are randomly assigned to the initial starting point, the process converges to the correct result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convergence Behaviour\n",
    "\n",
    "Having demonstrated gradient descent convergence of a simple convex function, let's investigate convergence behaviour\n",
    "on different functions when the parameters of the convergence process are manipulated.\n",
    "\n",
    "#### 3.1 Changing Max Iterations\n",
    "\n",
    "Clearly we have have a relationship between the function's gradient, the step size, and the number of iterations that can cause nonconvergence when max iterations is limited. \n",
    "\n",
    "We want the step size to be small enough such that the monotonic series is stable in its convergence to a minima,\n",
    "but 'small enough' assumes a sufficient series length. By severely limiting iterations, we can prevent convergence even on the simple function we've observed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_val, minimum_point = gradient_descent.gradient_descent(\n",
    "        gamma=0.01,\n",
    "        max_iterations=5,\n",
    "        f=f,\n",
    "    )\n",
    "print(\"\\nResults:\")\n",
    "print(f\"Min Value: {minimum_val}\")\n",
    "print(f\"Min Location: {minimum_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Changing Step Size\n",
    "\n",
    "The _Rosenbrock function_ function below was specifically designed to test optimization functions like gradient descent. The global minima of the function is $(1, 1)$ but is difficult to reach this minima because it is in a _very_\n",
    "shallow valley."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x,y)=(1-x)^2+100(y-x^2)^2$$\n",
    "\n",
    "alternate form\n",
    "\n",
    "$$100 x^4 - 200 x^2 y + x^2 - 2 x + 100 y^2 + 1$$\n",
    "\n",
    "We can use `matplotlib` to visualise the shallowness of the area in which the $(1, 1)$ global minima resides.\n",
    "Due to the sensitivity of our contour bands, the function's value area to be unchanging in a large parabolic region\n",
    "containing the minima. In fact, the area contains a _very slight_ decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.001\n",
    "x = np.arange(-3.0, 3.0, delta)\n",
    "y = np.arange(-2.0, 2.0, delta)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "Z1 = np.exp(-X**2 - Y**2)\n",
    "Z2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\n",
    "Z = (Z1 - Z2) * 2\n",
    "\n",
    "x = np.linspace(-2,2)\n",
    "y = np.linspace(-2,2)\n",
    "\n",
    "# Create our grid of points\n",
    "xv, yv = np.meshgrid(x,y)\n",
    "ax = plt.subplot(1,2,1) \n",
    "\n",
    "# Make a contour plot that is filled with color.\n",
    "ax.contourf(xv,yv, (1 - xv)**2+ 100*(yv - (xv**2))**2)\n",
    "ax.set_title('Contours of f(x,y)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting gradient descent on this function, we do not the global minimum even after a large number of iterations,\n",
    "_but we get close_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gradient_descent.Variable(\"x\")\n",
    "y = gradient_descent.Variable(\"y\")\n",
    "f = gradient_descent.MultiVariableFunction(                                                    # f(x, y) =\n",
    "    variables={x, y},\n",
    "    expressions=[\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=100, exponent=4),        # 100x^4 -\n",
    "        gradient_descent.Multiply(                                                             # 200x^2y +\n",
    "            a=gradient_descent.PolynomialExpression(variable=x, coefficient=-200, exponent=2),\n",
    "            b=gradient_descent.PolynomialExpression(variable=y, coefficient=1, exponent=1),\n",
    "        ),  \n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=1, exponent=2),          # x^2 -\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=-2, exponent=1),         # 2x +\n",
    "        gradient_descent.PolynomialExpression(variable=y, coefficient=100, exponent=2),        # 100y^2 +\n",
    "        gradient_descent.ConstantExpression(real=1.0),                                         # 1\n",
    "    ],\n",
    ")\n",
    "\n",
    "f.gradient()\n",
    "\n",
    "# The grad descent process on this function is quite sensitive to initial point. \n",
    "# If started too far away from minima, the gradients 'explode' and convergence does not occur.\n",
    "initial_points = [\n",
    "    {x: 1, y: 1},\n",
    "    {x: 1.1, y: 1.1},\n",
    "]\n",
    "\n",
    "MAX_ITERATIONS = 5000  # How big does this need to be to achieve convergence???\n",
    "\n",
    "for i_p in initial_points:\n",
    "    minimum_val, minimum_point = gradient_descent.gradient_descent(\n",
    "        gamma=0.00001,\n",
    "        max_iterations=MAX_ITERATIONS,\n",
    "        initial_point=i_p,\n",
    "        f=f,\n",
    "    )\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"--- Min Value: {minimum_val}\")\n",
    "    print(f\"--- Min Location: {minimum_point}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Changing initial starting points\n",
    "\n",
    "For functions with _multiple_ local minima, it matters which initial starting point we use, as gradient descent has\n",
    "no mathematical mechanism of exploring multiple local minima.\n",
    "\n",
    "The function below has two minima, at $(1, 1)$ and $(-1, -1)$. Gradient descent can find either, depending on initial point of descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x, y) = x^4 + y^4 - 4xy + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gradient_descent.Variable(\"x\")\n",
    "y = gradient_descent.Variable(\"y\")\n",
    "f = gradient_descent.MultiVariableFunction(                                                    # f(x, y) =\n",
    "    variables={x, y},\n",
    "    expressions=[\n",
    "        gradient_descent.PolynomialExpression(variable=x, coefficient=1, exponent=4),          # x^4 +\n",
    "        gradient_descent.PolynomialExpression(variable=y, coefficient=1, exponent=4),          # y^4 -\n",
    "        gradient_descent.Multiply(                                                             # 4xy +\n",
    "            a=gradient_descent.PolynomialExpression(variable=x, coefficient=-4, exponent=1),\n",
    "            b=gradient_descent.PolynomialExpression(variable=y, coefficient=1, exponent=1),\n",
    "        ),  \n",
    "        gradient_descent.ConstantExpression(real=1.0),                                         # 1\n",
    "    ],\n",
    ")\n",
    "\n",
    "# The grad descent process on this function is quite sensitive to initial point. \n",
    "# If started too far away from minima, the gradients 'explode' and convergence does not occur.\n",
    "initial_points = [\n",
    "    {x: 1, y: 1},\n",
    "    {x: -1, y: -1},\n",
    "    {x: -1.5, y: -1.5},\n",
    "    {x: 1.1, y: 1.2},\n",
    "]\n",
    "\n",
    "for i_p in initial_points:\n",
    "    minimum_val, minimum_point = gradient_descent.gradient_descent(\n",
    "        gamma=0.1,\n",
    "        max_iterations=50000,\n",
    "        initial_point=i_p,\n",
    "        f=f,\n",
    "    )\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"--- Min Value: {minimum_val}\")\n",
    "    print(f\"--- Min Location: {minimum_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this report we've demonstrated the core components of the gradient descent optimization process, in both Python and equations. Convergence has been demonstrated on simple polynomials and more complicated polynomials that can demonstrate how careful parameter tuning of the gradient descent process can be necessary to ensure convergence, even on convex functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*  RUMELHART, David E.; HINTON, Geoffrey E.; WILLIAMS, Ronald J. (1986). \"Learning representations by back-propagating errors\". Nature. 323 (6088): 533–536. doi:10.1038/323533a0. S2CID 205001834 -http://www.cs.utoronto.ca/~hinton/absps/naturebp.pdf\n",
    "* STEWART, J. (2019). \"Calculus: concepts and contexts\". Boston, MA, USA, Cengage.\n",
    "* SANDERSON, G. (2018). \"Why the gradient is the direction of steepest ascent\". https://www.youtube.com/watch?v=TEB2z7ZlRAw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "The full `gradient_descent` implementation is available online at [github.com/thundergolfer/modelling_change/](https://github.com/thundergolfer/modelling_change/), but it has also been copied in below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pure-Python3 implementation of Gradient Descent (https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "Written for educational/learning purposes and not performance.\n",
    "\n",
    "Completed as part of the UTS course '35512 - Modelling Change' (https://handbook.uts.edu.au/subjects/35512.html).\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import List, Mapping, Optional, Dict, Set, Tuple\n",
    "\n",
    "\n",
    "# Used to make chars like 'x' resemble typical mathematical symbols.\n",
    "def _italic_str(text: str) -> str:\n",
    "    return f\"\\x1B[3m{text}\\x1B[23m\"\n",
    "\n",
    "\n",
    "def _superscript_exp(n: str) -> str:\n",
    "    return \"\".join([\"⁰¹²³⁴⁵⁶⁷⁸⁹\"[ord(c) - ord('0')] for c in str(n)])\n",
    "\n",
    "\n",
    "class Variable:\n",
    "    \"\"\"\n",
    "    A object representing a mathematical variable, for use in building expressions.\n",
    "\n",
    "    Usage: `x = Variable(\"x\")`\n",
    "    \"\"\"\n",
    "    def __init__(self, var: str):\n",
    "        if len(var) != 1 or (not var.isalpha()):\n",
    "            raise ValueError(\"Variable must be single alphabetical character. eg. 'x'\")\n",
    "        self.var = var\n",
    "\n",
    "    def __repr__(self):\n",
    "        return _italic_str(self.var)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Overrides the default implementation\"\"\"\n",
    "        if isinstance(other, Variable):\n",
    "            return self.var == other.var\n",
    "        return False\n",
    "\n",
    "    def __key(self):\n",
    "        return self.var\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__key())\n",
    "\n",
    "\n",
    "# An element of some set called a space. Here, that 'space' will be the domain of a multi-variable function.\n",
    "Point = Dict[Variable, float]\n",
    "\n",
    "\n",
    "class Expression:\n",
    "    def diff(self, ref_var: Optional[Variable] = None) -> Optional[\"Expression\"]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, point: Point) -> float:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ConstantExpression(Expression):\n",
    "    \"\"\"\n",
    "    ConstantExpression is a single real-valued number.\n",
    "    It cannot be parameterised and its first-derivative is always 0 (None).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, real: float):\n",
    "        super().__init__()\n",
    "        self.real = real\n",
    "\n",
    "    def diff(self, ref_var: Optional[Variable] = None) -> Optional[Expression]:\n",
    "        return None\n",
    "\n",
    "    def evaluate(self, point: Point) -> float:\n",
    "        return self.real\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.real)\n",
    "\n",
    "\n",
    "class PolynomialExpression(Expression):\n",
    "    \"\"\"\n",
    "    An expression object that support evaluation and differentiation of single-variable polynomials.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            variable: Variable,\n",
    "            coefficient: float,\n",
    "            exponent: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.var = variable\n",
    "        self.coefficient = coefficient\n",
    "        self.exp = exponent\n",
    "\n",
    "    def diff(self, ref_var: Optional[Variable] = None) -> Optional[Expression]:\n",
    "        if ref_var and ref_var != self.var:\n",
    "            return None\n",
    "        if self.exp == 1:\n",
    "            return ConstantExpression(real=self.coefficient)\n",
    "        return PolynomialExpression(\n",
    "            variable=self.var,\n",
    "            coefficient=self.coefficient * self.exp,\n",
    "            exponent=self.exp - 1,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, point: Point) -> float:\n",
    "        return (\n",
    "                self.coefficient *\n",
    "                point[self.var] ** self.exp\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.coefficient}{self.var}{_superscript_exp(str(self.exp))}\"\n",
    "\n",
    "\n",
    "class Multiply(Expression):\n",
    "    def __init__(self, a: PolynomialExpression, b: PolynomialExpression):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def diff(self, ref_var: Optional[Variable] = None) -> Optional[\"Expression\"]:\n",
    "        if not ref_var:\n",
    "            raise RuntimeError(\"Must pass ref_var when differentiating Multiply expression\")\n",
    "        if self.a.var == ref_var:\n",
    "            diff_a = self.a.diff(ref_var=ref_var)\n",
    "            if not diff_a:\n",
    "                return None\n",
    "            else:\n",
    "                return Multiply(a=diff_a, b=self.b)\n",
    "        elif self.b.var == ref_var:\n",
    "            diff_b = self.b.diff(ref_var=ref_var)\n",
    "            if not diff_b:\n",
    "                return None\n",
    "            else:\n",
    "                return Multiply(a=self.a, b=diff_b)\n",
    "        else:\n",
    "            return None  # diff with respect to some non-involved variable is 0\n",
    "\n",
    "    def evaluate(self, point: Point) -> float:\n",
    "        return self.a.evaluate(point) * self.b.evaluate(point)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.a})({self.b})\"\n",
    "\n",
    "\n",
    "GradientVector = Dict[Variable, \"MultiVariableFunction\"]\n",
    "\n",
    "\n",
    "class MultiVariableFunction:\n",
    "    \"\"\"\n",
    "    MultiVariableFunction support the composition of expressions by addition into a\n",
    "    function of multiple real-valued variables.\n",
    "\n",
    "    Partial differentiation with respect to a single variable is supported, as is\n",
    "    evaluation at a Point, and gradient finding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, variables: Set[Variable], expressions: List[Expression]):\n",
    "        self.vars = variables\n",
    "        self.expressions = expressions\n",
    "\n",
    "    def gradient(self) -> GradientVector:\n",
    "        grad_v: GradientVector = {}\n",
    "        for v in self.vars:\n",
    "            grad_v[v] = self.diff(ref_var=v)\n",
    "        return grad_v\n",
    "\n",
    "    def diff(self, ref_var: Variable) -> \"MultiVariableFunction\":\n",
    "        first_partial_derivatives: List[Expression] = []\n",
    "        for expression in self.expressions:\n",
    "            first_partial_diff = expression.diff(ref_var=ref_var)\n",
    "            if first_partial_diff:\n",
    "                first_partial_derivatives.append(first_partial_diff)\n",
    "        return MultiVariableFunction(\n",
    "            variables=self.vars,\n",
    "            expressions=first_partial_derivatives,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, point: Point) -> float:\n",
    "        return sum(\n",
    "            expression.evaluate(point)\n",
    "            for expression\n",
    "            in self.expressions\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \" + \".join([str(e) for e in self.expressions])\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    gamma: float,\n",
    "    max_iterations: int,\n",
    "    f: MultiVariableFunction,\n",
    "    initial_point: Optional[Point] = None,\n",
    ") -> Tuple[float, Point]:\n",
    "    \"\"\"\n",
    "    Implements Gradient Descent (https://en.wikipedia.org/wiki/Gradient_descent) in pure-Python3.6+ with\n",
    "    no external dependencies.\n",
    "\n",
    "    :param gamma: 'step size', or 'learning rate'\n",
    "    :param max_iterations: Maximum number of steps in descent process.\n",
    "    :param f: A differentiable function off multiple real-valued variables.\n",
    "    :param initial_point: Optionally, a place to start the descent process\n",
    "    :return: A tuple of first a local minimum and second the point at which minimum is found.\n",
    "    \"\"\"\n",
    "    if gamma <= 0:\n",
    "        raise ValueError(\"gamma value must be a positive real number, γ∈ℝ+\")\n",
    "\n",
    "    iterations_per_logline = 100\n",
    "    a: Point = {}\n",
    "    f_grad = f.gradient()\n",
    "\n",
    "    if not initial_point:\n",
    "        for v in f.vars:\n",
    "            a[v] = random.randrange(4)\n",
    "    else:\n",
    "        a = initial_point\n",
    "    for i in range(max_iterations):\n",
    "        # Calculate function's gradient @ point `a`\n",
    "        grad_a: Mapping[Variable, float] = {\n",
    "            var: grad_elem.evaluate(a)\n",
    "            for var, grad_elem\n",
    "            in f_grad.items()\n",
    "        }\n",
    "        # update estimate of minimum point\n",
    "        a_next = {\n",
    "            var: current - (gamma * grad_a[var])\n",
    "            for var, current\n",
    "            in a.items()\n",
    "        }\n",
    "        a_prev = a\n",
    "        a = a_next\n",
    "\n",
    "        if a_prev == a:\n",
    "            print(\"Iteration as not changed value. Stopping early.\")\n",
    "            break\n",
    "        if i % iterations_per_logline == 0:\n",
    "            print(f\"Iteration {i}. Current min estimate: {a}\")\n",
    "    return f.evaluate(a), a\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    x = Variable(\"x\")\n",
    "    y = Variable(\"y\")\n",
    "\n",
    "    # Test variable comparisons\n",
    "    ##########################\n",
    "    assert Variable(\"x\") == Variable(\"x\")\n",
    "    assert Variable(\"x\") != Variable(\"y\")\n",
    "    assert Variable(\"y\") != Variable(\"x\")\n",
    "    assert Variable(\"y\") != Variable(\"z\")\n",
    "\n",
    "    # Test gradient evaluations of Expressions\n",
    "    ##########################################\n",
    "    # ConstantExpressions\n",
    "    assert ConstantExpression(real=0.0).diff() is None\n",
    "    assert ConstantExpression(real=4.5).diff() is None\n",
    "    # PolynomialExpression\n",
    "    poly1 = PolynomialExpression(\n",
    "        variable=Variable(\"x\"),\n",
    "        coefficient=2,\n",
    "        exponent=4,\n",
    "    )\n",
    "    poly1_grad1 = poly1.diff()\n",
    "    assert poly1_grad1.var == Variable(\"x\")\n",
    "    assert poly1_grad1.coefficient == 8\n",
    "    assert poly1_grad1.exp == 3\n",
    "    poly1_grad2 = poly1.diff(ref_var=Variable(\"y\"))\n",
    "    assert poly1_grad2 is None\n",
    "\n",
    "    # Test function evaluation\n",
    "    ##########################\n",
    "    x = Variable(\"x\")\n",
    "    y = Variable(\"y\")\n",
    "    # f = 3x + y^2\n",
    "    f1 = MultiVariableFunction(\n",
    "        variables={x, y},\n",
    "        expressions=[\n",
    "            PolynomialExpression(variable=x, coefficient=3, exponent=1),\n",
    "            PolynomialExpression(variable=y, coefficient=1, exponent=2),\n",
    "        ],\n",
    "    )\n",
    "    assert f1.evaluate(point={x: 1.0, y: 1.0}) == 4\n",
    "    assert f1.evaluate(point={x: 1.0, y: 2.0}) == 7\n",
    "    # Test function gradient\n",
    "    g = f1.gradient()\n",
    "    assert str(g[x]) == \"3\"\n",
    "\n",
    "    # Test Multiply\n",
    "    ##########################\n",
    "    a = PolynomialExpression(variable=x, coefficient=3, exponent=1)\n",
    "    b = PolynomialExpression(variable=y, coefficient=1, exponent=2)\n",
    "    a_times_b = Multiply(a=a, b=b)\n",
    "    result = a_times_b.evaluate(point={x: 2.0, y: 4.0})\n",
    "    assert result == (6 * 16)\n",
    "    result = a_times_b.evaluate(point={x: 3.0, y: 5.0})\n",
    "    assert result == 225\n",
    "    # Test diff on multiplication expression\n",
    "    a_times_b_diff = a_times_b.diff(ref_var=x)\n",
    "    assert a_times_b_diff.evaluate(point={x: 1.0, y: 5.0}) == 75\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
